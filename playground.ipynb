{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from costly import Costlog\n",
    "from solib.utils import *\n",
    "from solib.llm_utils import *\n",
    "from solib.protocols.common import *\n",
    "from solib.protocols.debate import *\n",
    "from solib.protocols.consultancy import *\n",
    "from solib.protocols.variants.common import *\n",
    "from solib.protocols.variants.debate import *\n",
    "from solib.protocols.variants.consultancy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.18) or chardet (5.2.0)/charset_normalizer (2.0.10) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'cost_min': 0.00054345, 'cost_max': 0.00054345, 'time_min': 12.068290999974124, 'time_max': 12.068290999974124, 'input_tokens': 35, 'output_tokens': 897, 'output_tokens_min': 897, 'output_tokens_max': 897, 'calls': 1, 'model': 'gpt-4o-mini', 'simulated': False, 'input_string': None, 'messages': None, 'output_string': 'The Riemann Hypothesis is one of the most famous and long-standing unsolved problems in mathematics, specifically in number theory and complex analysis. It is concerned with the distribution of prime numbers and is deeply connected to the properties of the Riemann zeta function.\\n\\n### Prerequisites\\n\\nTo understand the Riemann Hypothesis in detail, you should be familiar with the following concepts:\\n\\n1. **Complex Analysis**: Understanding complex functions, analytic functions, and the concept of meromorphic functions.\\n2. **Number Theory**: Familiarity with prime numbers, the distribution of primes, and the concept of the Euler product.\\n3. **Series and Products**: Knowledge of infinite series and products, particularly those that converge in the complex plane.\\n4. **Analytic Continuation**: The concept of extending the domain of a function beyond its initial definition.\\n5. **Functional Equation**: Understanding how certain functions can be transformed under specific operations.\\n\\n### The Riemann Zeta Function\\n\\nThe Riemann zeta function \\\\(\\\\zeta(s)\\\\) is defined for complex numbers \\\\(s\\\\) with real part greater than 1 by the series:\\n\\n\\\\[\\n\\\\zeta(s) = \\\\sum_{n=1}^{\\\\infty} \\\\frac{1}{n^s}\\n\\\\]\\n\\nThis series converges for \\\\(\\\\text{Re}(s) > 1\\\\). The zeta function can be analytically continued to other values of \\\\(s\\\\) (except for \\\\(s = 1\\\\), where it has a simple pole) using various techniques, including the use of the Euler product formula:\\n\\n\\\\[\\n\\\\zeta(s) = \\\\prod_{p \\\\text{ prime}} \\\\frac{1}{1 - p^{-s}}\\n\\\\]\\n\\nfor \\\\(\\\\text{Re}(s) > 1\\\\). This product representation connects the zeta function to the distribution of prime numbers.\\n\\n### The Functional Equation\\n\\nThe Riemann zeta function satisfies a functional equation that relates \\\\(\\\\zeta(s)\\\\) to \\\\(\\\\zeta(1-s)\\\\):\\n\\n\\\\[\\n\\\\zeta(s) = 2^s \\\\pi^{s-1} \\\\sin\\\\left(\\\\frac{\\\\pi s}{2}\\\\right) \\\\Gamma(1-s) \\\\zeta(1-s)\\n\\\\]\\n\\nwhere \\\\(\\\\Gamma(s)\\\\) is the gamma function. This equation is crucial for understanding the symmetry of the zeta function.\\n\\n### The Critical Strip and the Critical Line\\n\\nThe Riemann Hypothesis posits that all non-trivial zeros of the zeta function lie within the \"critical strip\" defined by \\\\(0 < \\\\text{Re}(s) < 1\\\\). More specifically, it conjectures that all non-trivial zeros have their real part equal to \\\\(\\\\frac{1}{2}\\\\). Thus, the hypothesis can be stated as:\\n\\n\\\\[\\n\\\\text{If } \\\\zeta(s) = 0 \\\\text{ and } 0 < \\\\text{Re}(s) < 1, \\\\text{ then } \\\\text{Re}(s) = \\\\frac{1}{2}.\\n\\\\]\\n\\n### Implications of the Riemann Hypothesis\\n\\nThe Riemann Hypothesis has profound implications for number theory, particularly in the distribution of prime numbers. The prime number theorem, which describes the asymptotic distribution of primes, can be refined using the Riemann Hypothesis. Specifically, it implies that the error term in the prime number theorem can be significantly reduced.\\n\\n### Current Status\\n\\nAs of now, the Riemann Hypothesis remains unproven, despite extensive numerical evidence supporting it. Many zeros of the zeta function have been computed, and all known non-trivial zeros lie on the critical line \\\\(\\\\text{Re}(s) = \\\\frac{1}{2}\\\\). The hypothesis is one of the seven \"Millennium Prize Problems,\" with a reward of one million dollars for a correct proof or counterexample.\\n\\n### Conclusion\\n\\nIn summary, the Riemann Hypothesis is a conjecture about the zeros of the Riemann zeta function, asserting that all non-trivial zeros lie on the critical line in the complex plane. Its proof or disproof would have significant consequences for number theory and our understanding of prime numbers. The study of the zeta function and its properties continues to be a rich area of research in mathematics.', 'description': None}]\n"
     ]
    }
   ],
   "source": [
    "ce = Costlog()\n",
    "x = get_llm_response(\n",
    "    prompt=\"Explain the Riemann Hypothesis to me in full mathematical detail, including all necessary prerequisites. Assume I have a strong background in mathematics.\",\n",
    "    model=\"gpt-4o-mini\",\n",
    "    simulate=False,\n",
    "    cost_log=ce,\n",
    ")\n",
    "print(ce.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = Question(\n",
    "    question = 'Who will be the 2024 presidential winner?',\n",
    "    possible_answers=[Answer('A', 'Donald Trump'), Answer('B', 'not Donald Trump')],\n",
    "    correct_answer=Answer('A', 'Donald Trump'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to tokenize input_string None or messages None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\costly\\estimators\\llm_api_estimation.py:419\u001b[0m, in \u001b[0;36mLLM_API_Estimation._get_tokens\u001b[1;34m(model, input_tokens, output_tokens_min, output_tokens_max, input_string, messages, output_string)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m input_string \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_string \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m SequentialDebate(\n\u001b[0;32m      2\u001b[0m     judge \u001b[38;5;241m=\u001b[39m Judge(),\n\u001b[0;32m      3\u001b[0m     debater_1 \u001b[38;5;241m=\u001b[39m Debater(),\n\u001b[0;32m      4\u001b[0m     debater_2 \u001b[38;5;241m=\u001b[39m Debater(),\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 6\u001b[0m dtrans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m d\u001b[38;5;241m.\u001b[39mrun(ques, cost_log\u001b[38;5;241m=\u001b[39mce, simulate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(dtrans)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(ce\u001b[38;5;241m.\u001b[39mitems)\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\math_problems_debate\\solib\\protocols\\debate.py:110\u001b[0m, in \u001b[0;36mSequentialDebate.run\u001b[1;34m(self, question, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m debater_1_answer, debater_2_answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_answers(question, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_communication(transcript, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 110\u001b[0m     debater_1_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebater_1(debater_1_answer, transcript, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    111\u001b[0m     transcript\u001b[38;5;241m.\u001b[39mappend(debater_1_item)\n\u001b[0;32m    112\u001b[0m     debater_2_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebater_2(debater_2_answer, transcript, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\math_problems_debate\\solib\\protocols\\debate.py:23\u001b[0m, in \u001b[0;36mDebater.__call__\u001b[1;34m(self, answer, transcript, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Subclasses should customize this.\"\"\"\u001b[39;00m\n\u001b[0;32m     22\u001b[0m words_in_mouth \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Sure, here\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms my response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 23\u001b[0m argument \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_llm_response_async(\n\u001b[0;32m     24\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mformat(answer\u001b[38;5;241m=\u001b[39manswer\u001b[38;5;241m.\u001b[39msymbol, transcript\u001b[38;5;241m=\u001b[39mtranscript),\n\u001b[0;32m     25\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m     26\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_wordceling,\n\u001b[0;32m     27\u001b[0m     words_in_mouth\u001b[38;5;241m=\u001b[39mwords_in_mouth,\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTranscriptItem(answer\u001b[38;5;241m=\u001b[39manswer, argument\u001b[38;5;241m=\u001b[39margument)\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\math_problems_debate\\solib\\llm_utils.py:520\u001b[0m, in \u001b[0;36mget_llm_response_async\u001b[1;34m(model, response_model, prompt, messages, input_string, system_message, words_in_mouth, max_tokens, temperature, **kwargs)\u001b[0m\n\u001b[0;32m    514\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    515\u001b[0m ai \u001b[38;5;241m=\u001b[39m get_llm(\n\u001b[0;32m    516\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    517\u001b[0m     use_async\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    518\u001b[0m     use_instructor\u001b[38;5;241m=\u001b[39m(response_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    519\u001b[0m )\n\u001b[1;32m--> 520\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ai[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerate_async\u001b[39m\u001b[38;5;124m\"\u001b[39m](\n\u001b[0;32m    521\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    522\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    523\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    524\u001b[0m     input_string\u001b[38;5;241m=\u001b[39minput_string,\n\u001b[0;32m    525\u001b[0m     system_message\u001b[38;5;241m=\u001b[39msystem_message,\n\u001b[0;32m    526\u001b[0m     words_in_mouth\u001b[38;5;241m=\u001b[39mwords_in_mouth,\n\u001b[0;32m    527\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39mmax_tokens,\n\u001b[0;32m    528\u001b[0m     response_model\u001b[38;5;241m=\u001b[39mresponse_model,\n\u001b[0;32m    529\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    530\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    531\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\costly\\decorator.py:54\u001b[0m, in \u001b[0;36mcostly.<locals>.decorator.<locals>.async_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m simulate:\n\u001b[0;32m     49\u001b[0m     simulator_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     50\u001b[0m         k: v\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m costly_kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature(simulator)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m     53\u001b[0m     } \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_log\u001b[39m\u001b[38;5;124m\"\u001b[39m: cost_log, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: description}\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimulator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msimulator_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cost_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m cost_log\u001b[38;5;241m.\u001b[39mnew_item_async() \u001b[38;5;28;01mas\u001b[39;00m (item, timer):\n",
      "File \u001b[1;32mc:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\costly\\simulators\\llm_simulator_faker.py:121\u001b[0m, in \u001b[0;36mLLM_Simulator_Faker.simulate_llm_call\u001b[1;34m(input_string, input_tokens, messages, model, response_model, cost_log, description)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel is required for tracking costs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cost_log\u001b[38;5;241m.\u001b[39mnew_item() \u001b[38;5;28;01mas\u001b[39;00m (item, _):\n\u001b[1;32m--> 121\u001b[0m         cost_item \u001b[38;5;241m=\u001b[39m \u001b[43mLLM_API_Estimation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cost_simulating\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# output_string=response, # not needed\u001b[39;49;00m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m         item\u001b[38;5;241m.\u001b[39mupdate(cost_item)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\costly\\estimators\\llm_api_estimation.py:464\u001b[0m, in \u001b[0;36mLLM_API_Estimation.get_cost_simulating\u001b[1;34m(model, input_tokens, output_tokens_min, output_tokens_max, input_string, messages, output_string, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cost_simulating\u001b[39m(\n\u001b[0;32m    454\u001b[0m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    462\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    463\u001b[0m     input_tokens, output_tokens_min, output_tokens_max \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 464\u001b[0m         \u001b[43mLLM_API_Estimation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_tokens_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_tokens_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_tokens_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_tokens_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m     )\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLM_API_Estimation\u001b[38;5;241m.\u001b[39m_get_cost_simulating_from_input_tokens_output_tokens(\n\u001b[0;32m    475\u001b[0m         input_tokens\u001b[38;5;241m=\u001b[39minput_tokens,\n\u001b[0;32m    476\u001b[0m         output_tokens_min\u001b[38;5;241m=\u001b[39moutput_tokens_min,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\costly\\estimators\\llm_api_estimation.py:427\u001b[0m, in \u001b[0;36mLLM_API_Estimation._get_tokens\u001b[1;34m(model, input_tokens, output_tokens_min, output_tokens_max, input_string, messages, output_string)\u001b[0m\n\u001b[0;32m    423\u001b[0m             input_tokens \u001b[38;5;241m=\u001b[39m LLM_API_Estimation\u001b[38;5;241m.\u001b[39mmessages_to_input_tokens(\n\u001b[0;32m    424\u001b[0m                 messages, model\n\u001b[0;32m    425\u001b[0m             )\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    428\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to tokenize input_string \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or messages \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m         )\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_tokens_min \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m output_tokens_max \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to tokenize input_string None or messages None"
     ]
    }
   ],
   "source": [
    "d = SequentialDebate(\n",
    "    judge = Judge(),\n",
    "    debater_1 = Debater(),\n",
    "    debater_2 = Debater(),\n",
    ")\n",
    "dtrans = await d.run(ques, cost_log=ce, simulate=True)\n",
    "print(dtrans)\n",
    "print(ce.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to tokenize input_string None or messages None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\costly\\estimators\\llm_api_estimation.py:419\u001b[0m, in \u001b[0;36mLLM_API_Estimation._get_tokens\u001b[1;34m(model, input_tokens, output_tokens_min, output_tokens_max, input_string, messages, output_string)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m input_string \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m input_string \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mAssertionError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m c \u001b[38;5;241m=\u001b[39m OpenConsultancy(\n\u001b[0;32m      2\u001b[0m     judge \u001b[38;5;241m=\u001b[39m Judge(),\n\u001b[0;32m      3\u001b[0m     consultant \u001b[38;5;241m=\u001b[39m Consultant(),\n\u001b[0;32m      4\u001b[0m     client\u001b[38;5;241m=\u001b[39mClient(),\n\u001b[0;32m      5\u001b[0m )\n\u001b[1;32m----> 6\u001b[0m ctrans \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m c\u001b[38;5;241m.\u001b[39mrun(ques, cost_log\u001b[38;5;241m=\u001b[39mce, simulate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(ctrans)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(ce\u001b[38;5;241m.\u001b[39mitems)\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\math_problems_debate\\solib\\protocols\\consultancy.py:167\u001b[0m, in \u001b[0;36mConsultancy.run\u001b[1;34m(self, question, **kwargs)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, question: Question, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Transcript:\n\u001b[0;32m    166\u001b[0m     transcript \u001b[38;5;241m=\u001b[39m Transcript(question, protocol\u001b[38;5;241m=\u001b[39mConsultancy)\n\u001b[1;32m--> 167\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchoose_answer(question, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsultant_goes_first:\n\u001b[0;32m    169\u001b[0m         consultant_item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsultant(answer, transcript, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\math_problems_debate\\solib\\protocols\\variants\\consultancy.py:10\u001b[0m, in \u001b[0;36mOpenConsultancy.choose_answer\u001b[1;34m(self, question, **kwargs)\u001b[0m\n\u001b[0;32m      8\u001b[0m auxillary_judge \u001b[38;5;241m=\u001b[39m Judge(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconsultant\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m      9\u001b[0m transcript \u001b[38;5;241m=\u001b[39m Transcript(question)\n\u001b[1;32m---> 10\u001b[0m auxillary_judgement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m auxillary_judge(transcript, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     11\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m auxillary_judgement\u001b[38;5;241m.\u001b[39mprobabilities\n\u001b[0;32m     12\u001b[0m ans_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(probabilities, key\u001b[38;5;241m=\u001b[39mprobabilities\u001b[38;5;241m.\u001b[39mget)\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\math_problems_debate\\solib\\protocols\\common.py:61\u001b[0m, in \u001b[0;36mJudge.__call__\u001b[1;34m(self, transcript, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, transcript: Transcript, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf.TranscriptItem\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     60\u001b[0m     words_in_mouth \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m I judge that the answer is:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 61\u001b[0m     probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m get_llm_probs_async(\n\u001b[0;32m     62\u001b[0m         prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt\u001b[38;5;241m.\u001b[39mformat(transcript\u001b[38;5;241m=\u001b[39mtranscript),\n\u001b[0;32m     63\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m     64\u001b[0m         return_probs_for\u001b[38;5;241m=\u001b[39mtranscript\u001b[38;5;241m.\u001b[39mquestion\u001b[38;5;241m.\u001b[39mpossible_answer_symbols,\n\u001b[0;32m     65\u001b[0m         words_in_mouth\u001b[38;5;241m=\u001b[39mwords_in_mouth,\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     67\u001b[0m     )\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mTranscriptItem(probabilities\u001b[38;5;241m=\u001b[39mprobabilities)\n",
      "File \u001b[1;32mc:\\Users\\abhim\\Google Drive\\Gittable\\Code\\math_problems_debate\\solib\\llm_utils.py:582\u001b[0m, in \u001b[0;36mget_llm_probs_async\u001b[1;34m(return_probs_for, model, prompt, messages, input_string, system_message, words_in_mouth, top_logprobs, temperature, **kwargs)\u001b[0m\n\u001b[0;32m    576\u001b[0m model \u001b[38;5;241m=\u001b[39m model \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4o-mini\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    577\u001b[0m ai \u001b[38;5;241m=\u001b[39m get_llm(\n\u001b[0;32m    578\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    579\u001b[0m     use_async\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    580\u001b[0m     use_instructor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    581\u001b[0m )\n\u001b[1;32m--> 582\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m ai[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_probs_async\u001b[39m\u001b[38;5;124m\"\u001b[39m](\n\u001b[0;32m    583\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    584\u001b[0m     return_probs_for\u001b[38;5;241m=\u001b[39mreturn_probs_for,\n\u001b[0;32m    585\u001b[0m     prompt\u001b[38;5;241m=\u001b[39mprompt,\n\u001b[0;32m    586\u001b[0m     messages\u001b[38;5;241m=\u001b[39mmessages,\n\u001b[0;32m    587\u001b[0m     input_string\u001b[38;5;241m=\u001b[39minput_string,\n\u001b[0;32m    588\u001b[0m     system_message\u001b[38;5;241m=\u001b[39msystem_message,\n\u001b[0;32m    589\u001b[0m     words_in_mouth\u001b[38;5;241m=\u001b[39mwords_in_mouth,\n\u001b[0;32m    590\u001b[0m     top_logprobs\u001b[38;5;241m=\u001b[39mtop_logprobs,\n\u001b[0;32m    591\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[0;32m    592\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    593\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\costly\\decorator.py:54\u001b[0m, in \u001b[0;36mcostly.<locals>.decorator.<locals>.async_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m simulate:\n\u001b[0;32m     49\u001b[0m     simulator_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     50\u001b[0m         k: v\n\u001b[0;32m     51\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m costly_kwargs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m signature(simulator)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m     53\u001b[0m     } \u001b[38;5;241m|\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcost_log\u001b[39m\u001b[38;5;124m\"\u001b[39m: cost_log, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m: description}\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msimulator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msimulator_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cost_log \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m cost_log\u001b[38;5;241m.\u001b[39mnew_item_async() \u001b[38;5;28;01mas\u001b[39;00m (item, timer):\n",
      "File \u001b[1;32mc:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\costly\\simulators\\llm_simulator_faker.py:121\u001b[0m, in \u001b[0;36mLLM_Simulator_Faker.simulate_llm_call\u001b[1;34m(input_string, input_tokens, messages, model, response_model, cost_log, description)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel is required for tracking costs\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m cost_log\u001b[38;5;241m.\u001b[39mnew_item() \u001b[38;5;28;01mas\u001b[39;00m (item, _):\n\u001b[1;32m--> 121\u001b[0m         cost_item \u001b[38;5;241m=\u001b[39m \u001b[43mLLM_API_Estimation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cost_simulating\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdescription\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# output_string=response, # not needed\u001b[39;49;00m\n\u001b[0;32m    128\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    129\u001b[0m         item\u001b[38;5;241m.\u001b[39mupdate(cost_item)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\costly\\estimators\\llm_api_estimation.py:464\u001b[0m, in \u001b[0;36mLLM_API_Estimation.get_cost_simulating\u001b[1;34m(model, input_tokens, output_tokens_min, output_tokens_max, input_string, messages, output_string, **kwargs)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_cost_simulating\u001b[39m(\n\u001b[0;32m    454\u001b[0m     model: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    462\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    463\u001b[0m     input_tokens, output_tokens_min, output_tokens_max \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 464\u001b[0m         \u001b[43mLLM_API_Estimation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_tokens\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    465\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    466\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    467\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_tokens_min\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_tokens_min\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    468\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_tokens_max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_tokens_max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    469\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_string\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    473\u001b[0m     )\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLM_API_Estimation\u001b[38;5;241m.\u001b[39m_get_cost_simulating_from_input_tokens_output_tokens(\n\u001b[0;32m    475\u001b[0m         input_tokens\u001b[38;5;241m=\u001b[39minput_tokens,\n\u001b[0;32m    476\u001b[0m         output_tokens_min\u001b[38;5;241m=\u001b[39moutput_tokens_min,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\abhim\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\costly\\estimators\\llm_api_estimation.py:427\u001b[0m, in \u001b[0;36mLLM_API_Estimation._get_tokens\u001b[1;34m(model, input_tokens, output_tokens_min, output_tokens_max, input_string, messages, output_string)\u001b[0m\n\u001b[0;32m    423\u001b[0m             input_tokens \u001b[38;5;241m=\u001b[39m LLM_API_Estimation\u001b[38;5;241m.\u001b[39mmessages_to_input_tokens(\n\u001b[0;32m    424\u001b[0m                 messages, model\n\u001b[0;32m    425\u001b[0m             )\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m--> 427\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    428\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to tokenize input_string \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or messages \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessages\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    429\u001b[0m         )\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_tokens_min \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m output_tokens_max \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to tokenize input_string None or messages None"
     ]
    }
   ],
   "source": [
    "c = OpenConsultancy(\n",
    "    judge = Judge(),\n",
    "    consultant = Consultant(),\n",
    "    client=Client(),\n",
    ")\n",
    "ctrans = await c.run(ques, cost_log=ce, simulate=True)\n",
    "print(ctrans)\n",
    "print(ce.items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(ce.items))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
